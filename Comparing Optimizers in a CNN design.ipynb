{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will be processing images with a CNN in a 2x2 experimental design. For this experiment I will use the train and test pattern with 10 epochs and a batch size of 64. I used 2 different combinations of activation functions (Sigmoid and Softmax) and optimizers (Adam and AdaDelta) and created 4 models. Each model was split into 80% Train and 20% Test dataset and the accuracy and loss was calculated on each dataset. The CNN model has 2 Convolutional layers, 2 Pooling layers, 1 Flatten and 2 Dense layers. All the images have been converted to 64 x 64 size and read as a Grayscale image.\n",
    "\n",
    "Here are the Results from Top two Models:\n",
    "\n",
    "1. Model 1 - Sigmoid activation function and Adam Optimizer - Final loss: 0.2458 - Train Accuracy: 0.9044 - val_loss: 1.0901 - Test Accuracy: 0.5925\n",
    "2. Model 2 - Sigmoid activation function and AdaDelta Optimizer - Final loss: 0.4120 - Train Accuracy: 0.8188 - val_loss: 0.7452 - Test Accuracy: 0.6150\n",
    "\n",
    "<b>Management Problem:</b>\n",
    "If accuracy is a priority then I will recommend using a CNN model with Sigmoid activation function and AdaDelta optimizer for Image Classification as the difference between Train and Test Accuracies is less and Loss in Test Dataset is lesser than that of Model 1. The CNN model should have 2 Convolutional layers, 2 Pooling layers, 1 Flatten and 2 Dense layers. Image size 64 X 64 works best for this model as per the experiment so it should be used for building the image classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Train and Test files for Cat and Dog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages to create CNN\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Activation, Conv2D, MaxPooling2D\n",
    "\n",
    "import os\n",
    "\n",
    "# Setting the path to cat dog image directory\n",
    "path = \"D:/Northwestern/DS 422/Assignment 7/cats_dogs_images/cats_dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the images into arrays\n",
    "# Dogs are categorized as 1 and Cats as 0\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "convert = lambda category : int(category == 'dog')\n",
    "def create_test_data(path):\n",
    "    for p in os.listdir(path):\n",
    "        category = p.split(\".\")[0]\n",
    "        category = convert(category)\n",
    "        img_array = cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "        new_img_array = cv2.resize(img_array, dsize=(64, 64))\n",
    "        X.append(new_img_array)\n",
    "        y.append(category)\n",
    "\n",
    "# Getting the images in an array and reshaping\n",
    "create_test_data(path)\n",
    "X = np.array(X).reshape(-1, 64,64,1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed\n",
    "RANDOM_SEED = 999\n",
    "\n",
    "# Normalize data\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Sigmoid activation function and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with 2 Convolutional and 2 Pooling layers\n",
    "# 1 Flatten and 2 Dense layers\n",
    "\n",
    "model = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Add another:\n",
    "model.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.6828 - acc: 0.6081 - val_loss: 0.8321 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.6663 - acc: 0.6250 - val_loss: 0.8515 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.6415 - acc: 0.6381 - val_loss: 0.9871 - val_acc: 0.0500\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5985 - acc: 0.6850 - val_loss: 0.8491 - val_acc: 0.4200\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5448 - acc: 0.7181 - val_loss: 1.1537 - val_acc: 0.2700\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4844 - acc: 0.7731 - val_loss: 0.8858 - val_acc: 0.4825\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.4410 - acc: 0.7956 - val_loss: 0.8870 - val_acc: 0.5600\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3640 - acc: 0.8400 - val_loss: 0.9287 - val_acc: 0.5775\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.3021 - acc: 0.8750 - val_loss: 1.1079 - val_acc: 0.5125\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.2458 - acc: 0.9044 - val_loss: 1.0901 - val_acc: 0.5925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f2eb06278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model for 10 epochs\n",
    "# Batch size 32\n",
    "# Creating a validation split of 20%\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final loss: 0.2458 - Train Accuracy: 0.9044 - val_loss: 1.0901 - Test Accuracy: 0.5925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Sigmoid activation function and AdaDelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with 2 Convolutional and 2 Pooling layers\n",
    "# 1 Flatten and 2 Dense layers\n",
    "\n",
    "model2 = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model2.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\n",
    "model2.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Add another:\n",
    "model2.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model2.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer=\"adadelta\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.6818 - acc: 0.6225 - val_loss: 1.1529 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.6648 - acc: 0.6250 - val_loss: 0.7885 - val_acc: 0.0075\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 23s 15ms/sample - loss: 0.6556 - acc: 0.6294 - val_loss: 0.8452 - val_acc: 0.0050\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 0.6328 - acc: 0.6444 - val_loss: 0.6855 - val_acc: 0.6900\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5972 - acc: 0.6637 - val_loss: 0.8146 - val_acc: 0.4300\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5906 - acc: 0.7038 - val_loss: 1.0114 - val_acc: 0.2625\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 0.5238 - acc: 0.7506 - val_loss: 0.7903 - val_acc: 0.5800\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.4915 - acc: 0.7744 - val_loss: 1.1712 - val_acc: 0.2950\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.4597 - acc: 0.7812 - val_loss: 1.3859 - val_acc: 0.2850\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 0.4120 - acc: 0.8188 - val_loss: 0.7452 - val_acc: 0.6150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f33406b38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model for 10 epochs\n",
    "# Batch size 32\n",
    "# Creating a validation split of 20%\n",
    "\n",
    "model2.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final loss: 0.4120 - Train Accuracy: 0.8188 - val_loss: 0.7452 - Test Accuracy: 0.6150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Softmax activation function and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with 2 Convolutional and 2 Pooling layers\n",
    "# 1 Flatten and 2 Dense layers\n",
    "\n",
    "model3 = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model3.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\n",
    "model3.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Add another:\n",
    "model3.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model3.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model3.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 22s 13ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f351bf358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model for 10 epochs\n",
    "# Batch size 32\n",
    "# Creating a validation split of 20%\n",
    "\n",
    "model3.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final loss: 9.9640 - Train Accuracy: 0.3750 - val_loss: 1.1921e-07 - Test Accuracy: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Softmax activation function and AdaDelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model with 2 Convolutional and 2 Pooling layers\n",
    "# 1 Flatten and 2 Dense layers\n",
    "\n",
    "model4 = Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model4.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\n",
    "model4.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Add another:\n",
    "model4.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "model4.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model4.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer=\"adadelta\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 24s 15ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 23s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "1600/1600 [==============================] - 22s 14ms/sample - loss: 9.9640 - acc: 0.3750 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22f37c32208>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model for 10 epochs\n",
    "# Batch size 32\n",
    "# Creating a validation split of 20%\n",
    "\n",
    "model4.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final loss: 9.9640 - Train Accuracy: 0.3750 - val_loss: 1.1921e-07 - Test Accuracy: 1.0000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
