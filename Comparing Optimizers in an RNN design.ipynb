{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will be processing text with a RNN in a 2x2 experimental design. The experiment design uses the train and test pattern with 50 epochs and a batch size of 100 and Adam Otimizer. Two word vector embeddings - glove.6B.50d.txt and glove.6B.100d.txt, were used along with two vocabulary sizes to create the following 2x2 experimental design:\n",
    "\n",
    "1. 10000 words with glove50d\n",
    "2. 20000 words with glove50d\n",
    "3. 10000 words with glove100d\n",
    "4. 20000 words with glove100d\n",
    "\n",
    "As the optional experiment, I used AdaDelta optimizer with 10000 and 20000 words from glove100d vector embedding, as Adam Optimizer had higher performance for this vector embedding.\n",
    "\n",
    "<b>Management Problem:</b>\n",
    "How would you advise senior management?\n",
    "Based on the results of this experiment, I would advise senior management to use a RNN Model having glove.6B.100d.txt embedding vector and Adam Optimizer. This model will be trained on existing customer feedback with a batch size of 100 taking 50 epochs. Using this training the model can identify the negative customer feelings. To reduce the processing time required by the model, we need to use the pretrained word vectors as a whole or just the subset. In the model that produces better results, we have used a subset of 20000 word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setting seed for reproducible results\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To remove all future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# Setting the embeddings directory\n",
    "embeddings_directory = 'embeddings/gloVe.6B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Importing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Function to create word dictionary with or without indexes.\n",
    "# It takes the embeddings file name and an flag\n",
    "# to check if indexes are needed.\n",
    "\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(filename):\n",
    "    # Getting the embeddings\n",
    "    embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "    \n",
    "    # Loading the embeddings\n",
    "    print('\\nLoading embeddings from', embeddings_filename)\n",
    "    word_to_index, index_to_embedding = \\\n",
    "        load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "    print(\"Embedding loaded from disks.\")\n",
    "    \n",
    "    # Getting the vocab size and embedding dimensions\n",
    "    embedding_dim = index_to_embedding.shape[1]\n",
    "    print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "    return word_to_index, index_to_embedding, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating utility functions to generate limited indexes and smaller vocab lists for the two different vocabulary sizes and data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create smaller vocab lists based on vocabulary size.\n",
    "\n",
    "def get_index_for_vocabsize(EVOCABSIZE, word_to_index,\n",
    "                            index_to_embedding, embedding_dim):\n",
    "    \n",
    "    # Need a callable function for dictionary\n",
    "    def default_factory():\n",
    "        return EVOCABSIZE\n",
    "    \n",
    "    # dictionary has the items() function, returns list of (key, value) tuples\n",
    "    limited_word_to_index = defaultdict(default_factory, \\\n",
    "        {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "    # Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "    limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "    # Set the unknown-word row to be all zeros as previously\n",
    "    limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "        index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "            reshape(1,embedding_dim), \n",
    "        axis = 0)\n",
    "    print(\"Limited indexes and word dict created with shape:\",\n",
    "          limited_index_to_embedding.shape)\n",
    "    return limited_word_to_index, limited_index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to get file names within a directory\n",
    "\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "\n",
    "codelist = ['\\r', '\\n', '\\t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text parsing function for creating text documents\n",
    "\n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data function that takes filename as a param and reads it into \n",
    "# a text list which is then split into words.\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup - Processing Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_movie_reviews(dir_end):\n",
    "\n",
    "    dir_start = '/Assignment 8/'\n",
    "    dir_name = os.path.join(dir_start, dir_end)\n",
    "    docs = []\n",
    "\n",
    "    filenames = listdir_no_hidden(path=dir_name)\n",
    "    num_files = len(filenames)\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "        assert file_exists\n",
    "    \n",
    "    print('\\nDirectory:',dir_end)\n",
    "    print('%d files found' % len(filenames))\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "        docs.append(words)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: run-jump-start-rnn-sentiment-v002/movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Directory: run-jump-start-rnn-sentiment-v002/movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# gather data for 500 negative movie reviews\n",
    "\n",
    "neg_dir_end = 'run-jump-start-rnn-sentiment-v002/movie-reviews-negative'\n",
    "\n",
    "negative_documents = gather_movie_reviews(neg_dir_end)\n",
    "\n",
    "# gather data for 500 positive movie reviews\n",
    "\n",
    "pos_dir_end = 'run-jump-start-rnn-sentiment-v002/movie-reviews-positive'\n",
    "\n",
    "positive_documents = gather_movie_reviews(pos_dir_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "# Testing the min max length of the movie reviews\n",
    "\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct list of 1000 lists with 40 words in each list\n",
    "\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RNN Model Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes Vocabulary size and Optimizer as parameters and returns the result dataframe after running the model for 50 epochs taking a batch size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_RNN_model(optimizer, limited_word_to_index, limited_index_to_embedding):\n",
    "\n",
    "    # create list of lists of lists for embeddings\n",
    "    embeddings = []    \n",
    "    for doc in documents:\n",
    "        embedding = []\n",
    "        for word in doc:\n",
    "            embedding.append(limited_index_to_embedding[\\\n",
    "                                                limited_word_to_index[word]]) \n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Make embeddings a numpy array for use in an RNN \n",
    "    # Create training and test sets with Scikit Learn\n",
    "\n",
    "    embeddings_array = np.array(embeddings)\n",
    "\n",
    "    # Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "    thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                          np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "    # Random splitting of the data in to training (80%) and test (20%)  \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                         random_state = RANDOM_SEED)\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # number of words per document\n",
    "    n_steps = embeddings_array.shape[1]\n",
    "    # dimension of  pre-trained embeddings\n",
    "    n_inputs = embeddings_array.shape[2]\n",
    "    n_neurons = 20  # analyst specified number of neurons\n",
    "    n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(states, n_outputs)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "    training_op = optimizer.minimize(loss)\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    n_epochs = 50\n",
    "    batch_size = 100\n",
    "\n",
    "    result = []\n",
    "    result_list = [None] * 50\n",
    "    e = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(y_train.shape[0] // batch_size):          \n",
    "                X_batch = X_train[iteration*batch_size:(iteration + 1)\\\n",
    "                                  *batch_size,:]\n",
    "                y_batch = y_train[iteration*batch_size:(iteration + 1)\\\n",
    "                                  *batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            result_list[epoch] = [acc_train,acc_test]\n",
    "    result_list_df = pd.DataFrame(result_list)\n",
    "    result_list_df.columns=['Train Accuracy','Test Accuracy']\n",
    "    return result_list_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the 2 x 2 design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 50)\n",
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.100d.txt\n",
      "Embedding loaded from disks.\n",
      "Embedding is of shape: (400001, 100)\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "OPT = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Vocabsize\n",
    "EVOCABSIZE1 = 10000\n",
    "EVOCABSIZE2 = 20000\n",
    "\n",
    "# Embedding\n",
    "FILENAME1 = 'glove.6B.50d.txt'\n",
    "FILENAME2 = 'glove.6B.100d.txt'\n",
    "\n",
    "word_to_index1, index_to_embedding1, embedding_dim1 = \\\n",
    "    get_embeddings(FILENAME1)\n",
    "word_to_index2, index_to_embedding2, embedding_dim2 = \\\n",
    "    get_embeddings(FILENAME2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 using Vocab size 10000, glove.6B.50d.txt embedding vector and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited indexes and word dict created with shape: (10001, 50)\n",
      "\n",
      "Mean Train and Test Accuracies from Model 1\n",
      "\n",
      "Train Accuracy    0.6710\n",
      "Test Accuracy     0.6162\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the limited word index array and embedding using vocabsize\n",
    "limited_word_to_index, limited_index_to_embedding = \\\n",
    "    get_index_for_vocabsize(EVOCABSIZE1, word_to_index1,\n",
    "                            index_to_embedding1, embedding_dim1)\n",
    "\n",
    "# Generate and run the model\n",
    "result_df_model1 = gen_RNN_model(OPT, limited_word_to_index,\n",
    "                                 limited_index_to_embedding)\n",
    "\n",
    "print('\\nMean Train and Test Accuracies from Model 1\\n')\n",
    "print(result_df_model1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 using Vocab size 20000, glove.6B.50d.txt embedding vector and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited indexes and word dict created with shape: (20001, 50)\n",
      "\n",
      "Mean Train and Test Accuracies from Model 2\n",
      "\n",
      "Train Accuracy    0.6772\n",
      "Test Accuracy     0.6084\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the limited word index array and embedding using vocabsize\n",
    "limited_word_to_index, limited_index_to_embedding = \\\n",
    "    get_index_for_vocabsize(EVOCABSIZE2, word_to_index1,\n",
    "                            index_to_embedding1, embedding_dim1)\n",
    "\n",
    "# Generate and run the model\n",
    "result_df_model2 = gen_RNN_model(OPT, limited_word_to_index,\n",
    "                                 limited_index_to_embedding)\n",
    "\n",
    "print('\\nMean Train and Test Accuracies from Model 2\\n')\n",
    "print(result_df_model2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 using Vocab size 10000, glove.6B.100d.txt embedding vector and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited indexes and word dict created with shape: (10001, 100)\n",
      "\n",
      "Mean Train and Test Accuracies from Model 3\n",
      "\n",
      "Train Accuracy    0.7522\n",
      "Test Accuracy     0.6233\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the limited word index array and embedding using vocabsize\n",
    "limited_word_to_index, limited_index_to_embedding = \\\n",
    "    get_index_for_vocabsize(EVOCABSIZE1, word_to_index2,\n",
    "                            index_to_embedding2, embedding_dim2)\n",
    "\n",
    "# Generate and run the model\n",
    "result_df_model3 = gen_RNN_model(OPT, limited_word_to_index,\n",
    "                                 limited_index_to_embedding)\n",
    "\n",
    "print('\\nMean Train and Test Accuracies from Model 3\\n')\n",
    "print(result_df_model3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 using Vocab size 20000, glove.6B.100d.txt embedding vector and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited indexes and word dict created with shape: (20001, 100)\n",
      "\n",
      "Mean Train and Test Accuracies from Model 4\n",
      "\n",
      "Train Accuracy    0.7898\n",
      "Test Accuracy     0.6108\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the limited word index array and embedding using vocabsize\n",
    "limited_word_to_index, limited_index_to_embedding = \\\n",
    "    get_index_for_vocabsize(EVOCABSIZE2, word_to_index2,\n",
    "                            index_to_embedding2, embedding_dim2)\n",
    "\n",
    "# Generate and run the model\n",
    "result_df_model4 = gen_RNN_model(OPT, limited_word_to_index,\n",
    "                                 limited_index_to_embedding)\n",
    "\n",
    "print('\\nMean Train and Test Accuracies from Model 4\\n')\n",
    "print(result_df_model4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---Testing two additional models with AdaDelta Optimizer---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT = tf.train.AdadeltaOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 using Vocab size 10000, glove.6B.100d.txt embedding vector and AdaDelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited indexes and word dict created with shape: (10001, 100)\n",
      "\n",
      "Mean Train and Test Accuracies from Model 5\n",
      "\n",
      "Train Accuracy    0.4600\n",
      "Test Accuracy     0.5342\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the limited word index array and embedding using vocabsize\n",
    "limited_word_to_index, limited_index_to_embedding = \\\n",
    "    get_index_for_vocabsize(EVOCABSIZE1, word_to_index2,\n",
    "                            index_to_embedding2, embedding_dim2)\n",
    "\n",
    "# Generate and run the model\n",
    "result_df_model5 = gen_RNN_model(OPT, limited_word_to_index,\n",
    "                                 limited_index_to_embedding)\n",
    "\n",
    "print('\\nMean Train and Test Accuracies from Model 5\\n')\n",
    "print(result_df_model5.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6 using Vocab size 20000, glove.6B.100d.txt embedding vector and AdaDelta Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited indexes and word dict created with shape: (20001, 100)\n",
      "\n",
      "Mean Train and Test Accuracies from Model 6\n",
      "\n",
      "Train Accuracy    0.460\n",
      "Test Accuracy     0.493\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Get the limited word index array and embedding using vocabsize\n",
    "limited_word_to_index, limited_index_to_embedding = \\\n",
    "    get_index_for_vocabsize(EVOCABSIZE2, word_to_index2,\n",
    "                            index_to_embedding2, embedding_dim2)\n",
    "\n",
    "# Generate and run the model\n",
    "result_df_model6 = gen_RNN_model(OPT, limited_word_to_index,\n",
    "                                 limited_index_to_embedding)\n",
    "\n",
    "print('\\nMean Train and Test Accuracies from Model 6\\n')\n",
    "print(result_df_model6.mean())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
