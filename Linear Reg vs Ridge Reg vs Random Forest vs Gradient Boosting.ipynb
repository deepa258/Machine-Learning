{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will be fitting regression models to data from the Boston Housing Study. All the explanatory variables (except neighborhood) and all 506 census tract observations from the Boston Housing Study will be used for this analysis. I will be using the median value of homes in thousands of 1970 dollars. The four regression modeling methods used here will be:\n",
    "\n",
    "1. Linear regression\n",
    "2. Ridge regression\n",
    "3. Random Forests\n",
    "4. Gradient Boosting\n",
    "\n",
    "These methods are evaluated within a cross-validation design, using root-mean-squared error (RMSE) as an index of prediction error.\n",
    "\n",
    "After analyzing the results from the four methods, I would suggest using the Random Forests methods over others for assessing the Market Value of residential real estate. This is because the RMSE and R-Squared values obtained in the cross-validation design show that Random Forests method is performing better and has a better fit to the model data as per the Accuracy results on training and test datasets.\n",
    "\n",
    "The two most important features in Boston data are lstat - \"Percentage of population of lower socio-economic status\" and rooms - \"Average number of rooms per home\" for predicting the Median Value of homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed to obtain reproducible results\n",
    "\n",
    "RANDOM_SEED = 1\n",
    "\n",
    "# Fitting the intercept term in the models\n",
    "\n",
    "SET_FIT_INTERCEPT = True\n",
    "\n",
    "# Importing base packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing Modeling routines from Scikit Learn packages\n",
    "\n",
    "import sklearn.linear_model \n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score  \n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General description of the boston_input DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "neighborhood    506 non-null object\n",
      "crim            506 non-null float64\n",
      "zn              506 non-null float64\n",
      "indus           506 non-null float64\n",
      "chas            506 non-null int64\n",
      "nox             506 non-null float64\n",
      "rooms           506 non-null float64\n",
      "age             506 non-null float64\n",
      "dis             506 non-null float64\n",
      "rad             506 non-null int64\n",
      "tax             506 non-null int64\n",
      "ptratio         506 non-null float64\n",
      "lstat           506 non-null float64\n",
      "mv              506 non-null float64\n",
      "dtypes: float64(10), int64(3), object(1)\n",
      "memory usage: 55.4+ KB\n",
      "None\n",
      "\n",
      "Descriptive statistics of the boston DataFrame:\n",
      "             crim          zn       indus        chas         nox       rooms  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
      "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
      "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
      "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
      "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
      "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
      "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
      "\n",
      "              age         dis         rad         tax     ptratio       lstat  \\\n",
      "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
      "mean    68.574901    3.795043    9.549407  408.237154   18.455534   12.653063   \n",
      "std     28.148861    2.105710    8.707259  168.537116    2.164946    7.141062   \n",
      "min      2.900000    1.129600    1.000000  187.000000   12.600000    1.730000   \n",
      "25%     45.025000    2.100175    4.000000  279.000000   17.400000    6.950000   \n",
      "50%     77.500000    3.207450    5.000000  330.000000   19.050000   11.360000   \n",
      "75%     94.075000    5.188425   24.000000  666.000000   20.200000   16.955000   \n",
      "max    100.000000   12.126500   24.000000  711.000000   22.000000   37.970000   \n",
      "\n",
      "               mv  \n",
      "count  506.000000  \n",
      "mean    22.528854  \n",
      "std      9.182176  \n",
      "min      5.000000  \n",
      "25%     17.025000  \n",
      "50%     21.200000  \n",
      "75%     25.000000  \n",
      "max     50.000000  \n"
     ]
    }
   ],
   "source": [
    "# Reading data for the Boston Housing Study\n",
    "\n",
    "boston_input = pd.read_csv('boston.csv')\n",
    "\n",
    "# Displaying attribute list in Boston DataFrame\n",
    "# This is necessary to check the attribute types and \n",
    "# If the fields can contain Null data or not\n",
    "\n",
    "print('\\nGeneral description of the boston_input DataFrame:')\n",
    "print(boston_input.info())\n",
    "\n",
    "# Dropping neighborhood attribute from the data being considered\n",
    "\n",
    "boston = boston_input.drop('neighborhood', 1)\n",
    "\n",
    "# Displaying descriptive stats of Boston DataFrame\n",
    "\n",
    "print('\\nDescriptive statistics of the boston DataFrame:')\n",
    "print(boston.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data dimensions: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "# Setting up the preliminary data model \n",
    "# the first column is the median housing value response\n",
    "# the remaining columns are the explanatory variables\n",
    "\n",
    "prelim_model_data = np.array(boston.reindex_axis(['mv'] + \\\n",
    "                                                 list(boston.columns[:-1]),\n",
    "                                                 axis = 1))\n",
    "\n",
    "# Looking at the data before standardization\n",
    "\n",
    "print('\\nData dimensions:', prelim_model_data.shape)\n",
    "\n",
    "# Importing StandardScaler to standardize the model data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# the model data will be standardized form of preliminary model data\n",
    "\n",
    "model_data = scaler.fit_transform(prelim_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing variations of Gradient Boosting method and Random Forests to get the models having good accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Train Test Split to split the dataset \n",
    "# with 20% test and 80% train data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainingSet, testSet = train_test_split(model_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating X and y for testing and training\n",
    "\n",
    "train_rows = trainingSet.shape[0]-1\n",
    "train_cols = trainingSet.shape[1]\n",
    "test_rows = testSet.shape[0]-1\n",
    "test_cols = testSet.shape[1]\n",
    "\n",
    "X_train = trainingSet[0:train_rows,1:train_cols]\n",
    "y_train = trainingSet[0:train_rows,0]\n",
    "X_test = testSet[0:test_rows,1:test_cols]\n",
    "y_test = testSet[0:test_rows,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Depth = 1\n",
      "Accuracy on training set: 0.883\n",
      "Accuracy on test set: 0.830\n",
      "Gradient Learning Rate = 0.01\n",
      "Accuracy on training set: 0.763\n",
      "Accuracy on test set: 0.677\n",
      "Gradient Boosting Depth = 5\n",
      "Accuracy on training set: 0.997\n",
      "Accuracy on test set: 0.884\n",
      "Gradient Learning Rate = 1\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.795\n",
      "Random Forest Depth = 5\n",
      "Accuracy on training set: 0.936\n",
      "Accuracy on test set: 0.853\n",
      "Random Forest Max Features = 5\n",
      "Accuracy on training set: 0.981\n",
      "Accuracy on test set: 0.882\n",
      "Random Forest Max Leaf Nodes = 5\n",
      "Accuracy on training set: 0.820\n",
      "Accuracy on test set: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Comparing the accuracies of various models on train and test sets\n",
    "        \n",
    "gbrt = GradientBoostingRegressor(random_state = RANDOM_SEED, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting Depth = 1\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train,\n",
    "                                                           y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
    "\n",
    "gbrt1 = GradientBoostingRegressor(random_state=RANDOM_SEED,\n",
    "                                  learning_rate=0.01)\n",
    "gbrt1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Learning Rate = 0.01\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt1.score(X_train,\n",
    "                                                            y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt1.score(X_test, y_test)))\n",
    "\n",
    "gbrt2 = GradientBoostingRegressor(random_state = RANDOM_SEED, max_depth=5)\n",
    "gbrt2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting Depth = 5\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt2.score(X_train,\n",
    "                                                            y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt2.score(X_test, y_test)))\n",
    "\n",
    "gbrt3 = GradientBoostingRegressor(random_state=RANDOM_SEED, learning_rate=1)\n",
    "gbrt3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Learning Rate = 1\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt3.score(X_train,\n",
    "                                                            y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt3.score(X_test, y_test)))\n",
    "\n",
    "rf = RandomForestRegressor(max_depth=5, random_state=RANDOM_SEED,\n",
    "                                    n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Depth = 5\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, y_test)))\n",
    "\n",
    "rf2 = RandomForestRegressor(max_features=5, random_state=RANDOM_SEED,\n",
    "                                    n_estimators=100)\n",
    "rf2.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Max Features = 5\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(rf2.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf2.score(X_test, y_test)))\n",
    "\n",
    "rf3 = RandomForestRegressor(max_leaf_nodes=5, random_state=RANDOM_SEED,\n",
    "                                    n_estimators=100)\n",
    "rf3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Max Leaf Nodes = 5\")\n",
    "print(\"Accuracy on training set: {:.3f}\".format(rf3.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the comparison of Accuracy results, it can be said that the methods Gradient Boosting with Depth = 5, Random Forest with Depth = 5 and Random Forest with Max Features = 5 are giving better performance and I will be using these methods for my cross validation design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression of Data using Linear Regression, Ridge Regression, Random Forest and Gradient Boosting in the K-Fold design and Cross-Validation of the Models using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting names list containing names of the regression models\n",
    "# Setting the regressors list specifying the regression models\n",
    "# In Ridge regression, we are keeping alpha = 1\n",
    "\n",
    "names = ['Linear_Regression', 'Ridge_Regression alpha=1',\n",
    "         'Random Forest Depth = 5', 'Random Forest Max Features = 5', \n",
    "         'Gradient Boosting Depth = 5'\n",
    "        ]\n",
    "\n",
    "regressors = [LinearRegression(fit_intercept = SET_FIT_INTERCEPT, \n",
    "              normalize = False), \n",
    "              Ridge(alpha = 1, solver = 'cholesky', \n",
    "                    fit_intercept = SET_FIT_INTERCEPT, \n",
    "                    normalize = False, \n",
    "                    random_state = RANDOM_SEED),\n",
    "              RandomForestRegressor(max_depth=5, random_state=RANDOM_SEED,\n",
    "                                    n_estimators=100),\n",
    "              RandomForestRegressor(max_features=5, random_state=RANDOM_SEED,\n",
    "                                    n_estimators=100),\n",
    "              GradientBoostingRegressor(random_state=RANDOM_SEED, max_depth=5)\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE for 10 FOLDS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear_Regression</th>\n",
       "      <th>Ridge_Regression alpha=1</th>\n",
       "      <th>Random Forest Depth = 5</th>\n",
       "      <th>Random Forest Max Features = 5</th>\n",
       "      <th>Gradient Boosting Depth = 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.308034</td>\n",
       "      <td>0.306744</td>\n",
       "      <td>0.373782</td>\n",
       "      <td>0.332930</td>\n",
       "      <td>0.308764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.414889</td>\n",
       "      <td>0.412916</td>\n",
       "      <td>0.239140</td>\n",
       "      <td>0.244068</td>\n",
       "      <td>0.235610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.438938</td>\n",
       "      <td>0.435998</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>0.199842</td>\n",
       "      <td>0.289151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.652244</td>\n",
       "      <td>0.653196</td>\n",
       "      <td>0.519690</td>\n",
       "      <td>0.528031</td>\n",
       "      <td>0.733590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621783</td>\n",
       "      <td>0.620399</td>\n",
       "      <td>0.380280</td>\n",
       "      <td>0.367053</td>\n",
       "      <td>0.372444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.496012</td>\n",
       "      <td>0.495756</td>\n",
       "      <td>0.546168</td>\n",
       "      <td>0.456488</td>\n",
       "      <td>0.495089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.343828</td>\n",
       "      <td>0.341483</td>\n",
       "      <td>0.329878</td>\n",
       "      <td>0.326521</td>\n",
       "      <td>0.278535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.340827</td>\n",
       "      <td>1.339664</td>\n",
       "      <td>1.038800</td>\n",
       "      <td>0.956999</td>\n",
       "      <td>0.977591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.669705</td>\n",
       "      <td>0.669238</td>\n",
       "      <td>0.617151</td>\n",
       "      <td>0.446687</td>\n",
       "      <td>0.676413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.333138</td>\n",
       "      <td>0.329718</td>\n",
       "      <td>0.397976</td>\n",
       "      <td>0.380154</td>\n",
       "      <td>0.416641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.561940</td>\n",
       "      <td>0.560511</td>\n",
       "      <td>0.467689</td>\n",
       "      <td>0.423877</td>\n",
       "      <td>0.478383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Linear_Regression  Ridge_Regression alpha=1  Random Forest Depth = 5  \\\n",
       "0              0.308034                  0.306744                 0.373782   \n",
       "1              0.414889                  0.412916                 0.239140   \n",
       "2              0.438938                  0.435998                 0.234021   \n",
       "3              0.652244                  0.653196                 0.519690   \n",
       "4              0.621783                  0.620399                 0.380280   \n",
       "5              0.496012                  0.495756                 0.546168   \n",
       "6              0.343828                  0.341483                 0.329878   \n",
       "7              1.340827                  1.339664                 1.038800   \n",
       "8              0.669705                  0.669238                 0.617151   \n",
       "9              0.333138                  0.329718                 0.397976   \n",
       "mean           0.561940                  0.560511                 0.467689   \n",
       "\n",
       "      Random Forest Max Features = 5  Gradient Boosting Depth = 5  \n",
       "0                           0.332930                     0.308764  \n",
       "1                           0.244068                     0.235610  \n",
       "2                           0.199842                     0.289151  \n",
       "3                           0.528031                     0.733590  \n",
       "4                           0.367053                     0.372444  \n",
       "5                           0.456488                     0.495089  \n",
       "6                           0.326521                     0.278535  \n",
       "7                           0.956999                     0.977591  \n",
       "8                           0.446687                     0.676413  \n",
       "9                           0.380154                     0.416641  \n",
       "mean                        0.423877                     0.478383  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifying the folds for validation - 10-fold cross-validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Specifying the folds for validation - 10-fold cross-validation\n",
    "\n",
    "N_FOLDS = 10\n",
    "\n",
    "# Setting up numpy array for storing results\n",
    "\n",
    "cv_results = np.zeros((N_FOLDS, len(names)))\n",
    "cv_results_r2 = np.zeros((N_FOLDS, len(names)))\n",
    "\n",
    "# Setting the KFold object using number of folds and \n",
    "# random seed set previously\n",
    "\n",
    "kf = KFold(n_splits = N_FOLDS, shuffle=False, random_state = RANDOM_SEED)\n",
    "\n",
    "# Checking the splitting process by looking at fold observation counts\n",
    "# Initializing the fold count and looping through the splits of model data\n",
    "# The variable index_for_fold will be used as the results array index\n",
    "\n",
    "index_for_fold = 0 \n",
    "\n",
    "# This loop will run 10 times, once for each fold\n",
    "\n",
    "for train_index, test_index in kf.split(model_data):\n",
    "        \n",
    "#   1:model_data.shape[1] slices for explanatory variables\n",
    "#   and 0 is the index for the response variable \n",
    "#   Storing the split index in a variable to avoid recalculation\n",
    "\n",
    "    model_data_exp = model_data.shape[1]\n",
    "\n",
    "# Generating Train and Test datasets for the current fold\n",
    "   \n",
    "    X_train = model_data[train_index, 1:model_data_exp]\n",
    "    X_test = model_data[test_index, 1:model_data_exp]\n",
    "    y_train = model_data[train_index, 0]\n",
    "    y_test = model_data[test_index, 0]   \n",
    "\n",
    "# Initializing the method count and looping through the methods \n",
    "# (Linear Regression and Ridge Regression)\n",
    "# This loop will run twice, once for each method, per fold\n",
    "# The variable index_for_method will be used as results array index\n",
    "    \n",
    "    index_for_method = 0\n",
    "    for name, reg_model in zip(names, regressors):\n",
    "        \n",
    "        reg_model.fit(X_train, y_train)\n",
    "        \n",
    "        # evaluate on the test set for this fold\n",
    "        y_test_predict = reg_model.predict(X_test)\n",
    "        \n",
    "        cv_results_r2[index_for_fold, \n",
    "                      index_for_method] = r2_score(y_test, y_test_predict)\n",
    "        \n",
    "        fold_method_result = sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "        \n",
    "        cv_results[index_for_fold, index_for_method] = fold_method_result\n",
    "        index_for_method += 1\n",
    "  \n",
    "    index_for_fold += 1\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "cv_results_df.columns = names\n",
    "\n",
    "cv_results_df_r2 = pd.DataFrame(cv_results_r2)\n",
    "cv_results_df_r2.columns = names\n",
    "\n",
    "print('\\nRMSE for 10 FOLDS\\n')\n",
    "\n",
    "cv_results_df.append(cv_results_df.mean().T,\n",
    "                     ignore_index=True).rename(index={10:'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-Squared for 10 FOLDS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear_Regression</th>\n",
       "      <th>Ridge_Regression alpha=1</th>\n",
       "      <th>Random Forest Depth = 5</th>\n",
       "      <th>Random Forest Max Features = 5</th>\n",
       "      <th>Gradient Boosting Depth = 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765075</td>\n",
       "      <td>0.767038</td>\n",
       "      <td>0.654085</td>\n",
       "      <td>0.725566</td>\n",
       "      <td>0.763960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.460676</td>\n",
       "      <td>0.465792</td>\n",
       "      <td>0.820820</td>\n",
       "      <td>0.813359</td>\n",
       "      <td>0.826071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.311175</td>\n",
       "      <td>-1.280315</td>\n",
       "      <td>0.343045</td>\n",
       "      <td>0.520930</td>\n",
       "      <td>-0.002942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.632886</td>\n",
       "      <td>0.631812</td>\n",
       "      <td>0.766939</td>\n",
       "      <td>0.759397</td>\n",
       "      <td>0.535605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.534229</td>\n",
       "      <td>0.536301</td>\n",
       "      <td>0.825778</td>\n",
       "      <td>0.837688</td>\n",
       "      <td>0.832884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.724891</td>\n",
       "      <td>0.725174</td>\n",
       "      <td>0.666440</td>\n",
       "      <td>0.766987</td>\n",
       "      <td>0.725914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.378268</td>\n",
       "      <td>0.386722</td>\n",
       "      <td>0.427695</td>\n",
       "      <td>0.439283</td>\n",
       "      <td>0.591983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.014566</td>\n",
       "      <td>-0.012806</td>\n",
       "      <td>0.391026</td>\n",
       "      <td>0.483158</td>\n",
       "      <td>0.460677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.041001</td>\n",
       "      <td>-1.038159</td>\n",
       "      <td>-0.733244</td>\n",
       "      <td>0.092007</td>\n",
       "      <td>-1.082091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.477957</td>\n",
       "      <td>0.488621</td>\n",
       "      <td>0.254972</td>\n",
       "      <td>0.320204</td>\n",
       "      <td>0.183452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.160724</td>\n",
       "      <td>0.167018</td>\n",
       "      <td>0.441755</td>\n",
       "      <td>0.575858</td>\n",
       "      <td>0.383551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Linear_Regression  Ridge_Regression alpha=1  Random Forest Depth = 5  \\\n",
       "0              0.765075                  0.767038                 0.654085   \n",
       "1              0.460676                  0.465792                 0.820820   \n",
       "2             -1.311175                 -1.280315                 0.343045   \n",
       "3              0.632886                  0.631812                 0.766939   \n",
       "4              0.534229                  0.536301                 0.825778   \n",
       "5              0.724891                  0.725174                 0.666440   \n",
       "6              0.378268                  0.386722                 0.427695   \n",
       "7             -0.014566                 -0.012806                 0.391026   \n",
       "8             -1.041001                 -1.038159                -0.733244   \n",
       "9              0.477957                  0.488621                 0.254972   \n",
       "mean           0.160724                  0.167018                 0.441755   \n",
       "\n",
       "      Random Forest Max Features = 5  Gradient Boosting Depth = 5  \n",
       "0                           0.725566                     0.763960  \n",
       "1                           0.813359                     0.826071  \n",
       "2                           0.520930                    -0.002942  \n",
       "3                           0.759397                     0.535605  \n",
       "4                           0.837688                     0.832884  \n",
       "5                           0.766987                     0.725914  \n",
       "6                           0.439283                     0.591983  \n",
       "7                           0.483158                     0.460677  \n",
       "8                           0.092007                    -1.082091  \n",
       "9                           0.320204                     0.183452  \n",
       "mean                        0.575858                     0.383551  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nR-Squared for 10 FOLDS\\n')\n",
    "\n",
    "cv_results_df_r2.append(cv_results_df_r2.mean().T,\n",
    "                        ignore_index=True).rename(index={10:'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean RMSE value of Random Forests is better (lower) than Gradient Boosting, Ridge Regression and Linear Regression and the R-Squared values of Random Forests are better (higher) than Gradient Boosting, Ridge Regression and Linear Regression. This shows that Random Forests have better performance on the data and can do a better job at predicting the value of the response variable (the median value of homes) using the explanatory variables. Out of the two Random Forest methods that I have tried the model with Max Features 5 gives better performance stats than the one with Depth 5. However, this could differ on different sets of data and the results could change with changes to data but overall, I would recommend using Random Forests as the Machine Learning method over other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature lstat (0.358594)\n",
      "2. feature rooms (0.319967)\n",
      "3. feature dis (0.059341)\n",
      "4. feature crim (0.058158)\n",
      "5. feature nox (0.052361)\n",
      "6. feature ptratio (0.044856)\n",
      "7. feature indus (0.043893)\n",
      "8. feature age (0.026910)\n",
      "9. feature tax (0.024183)\n",
      "10. feature rad (0.006871)\n",
      "11. feature zn (0.002950)\n",
      "12. feature chas (0.001917)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XucXVV99/HPl3C/hVu0QgJJMKBBEWhEnmKRyi1AuWjBxkuLSkUsVC2PVbAUNV4eROvraSsqIFEeFIOCYrSxSFFoRYEMdwJSknDJEISBcImAkITv88deAyfDzJyd5OzJIfm+X6/zmr3X3nut3zkzr/mdtdfea8s2ERERw1lvTQcQERHdL8kiIiLaSrKIiIi2kiwiIqKtJIuIiGgrySIiItpKsohYBZK+Iemf1nQcESNFuc8iRpKke4FXAstbinexvWg16twf+I7tsasX3cuTpG8DvbZPX9OxxNorPYtYE46wvXnLa5UTRSdIWn9Ntr86JI1a0zHEuiHJIrqGpH0k/VrS45JuKT2G/m3vk3SnpCWSFkj6YCnfDPgZsL2k35fX9pK+LelzLcfvL6m3Zf1eSZ+QdCvwlKT1y3GXSuqTdI+kDw8T6wv199ct6eOSHpb0oKSjJR0m6X8kLZb0yZZjPy3pEkkXl/dzo6Q3tGx/raSryucwV9KRA9r9uqTZkp4CjgfeDXy8vPeflP1OlTS/1H+HpLe11PFeSb+S9GVJj5X3emjL9m0kfUvSorL9spZtfy7p5hLbryXt3rLtE5IeKG3eJemAGr/2eLmwnVdeI/YC7gUOHKR8B+BR4DCqLzEHlfUxZfvhwM6AgLcATwN7lW37U52Gaa3v28DnWtZX2KfEcTMwDtiktHkDcAawITARWAAcMsT7eKH+UveycuwGwAeAPuAiYAtgN+APwMSy/6eBpcAxZf+PAfeU5Q2AecAnSxxvBZYAu7a0+wSwb4l544Hvtex3LLB92ecvgaeAV5Vt7y3tfwAYBXwIWMSLp6X/HbgY2LrE85ZSvhfwMPCmctxx5XPcCNgVWAhsX/YdD+y8pv/e8urcKz2LWBMuK99MH2/51voeYLbt2baft30F0EOVPLD977bnu3I18HPgT1czjn+1vdD2M8AbqRLTdNvP2V4AnAdMq1nXUuDztpcCM4HtgH+xvcT2XGAusHvL/jfYvqTs/xWqf/r7lNfmwJkljl8APwXe2XLsj21fUz6nPwwWjO0f2F5U9rkYuBvYu2WX+2yfZ3s5cAHwKuCVkl4FHAqcaPsx20vL5w1VcjnH9nW2l9u+AHi2xLycKmlMlrSB7Xttz6/52cXLQJJFrAlH296qvI4uZTsBx7YkkceBN1P9E0PSoZKuLad0HqdKItutZhwLW5Z3ojqV1dr+J6kG4+t4tPzjBXim/HyoZfszVEngJW3bfh7opeoJbA8sLGX97qPqeQ0W96Ak/XXL6aLHgdex4uf1u5b2ny6Lm1P1tBbbfmyQancC/veAz2gcVW9iHvBRql7Tw5JmStq+XZzx8pFkEd1iIXBhSxLZyvZmts+UtBFwKfBl4JW2twJmU52SAhjskr6ngE1b1v9okH1aj1sI3DOg/S1sH7ba72xw4/oXJK0HjKU6FbQIGFfK+u0IPDBE3C9Zl7QTVa/oZGDb8nndzouf13AWAttI2mqIbZ8f8Bltavt7ALYvsv1mqqRi4Is12ouXiSSL6BbfAY6QdIikUZI2LgPHY6nO3W9ENQ6wrAzGHtxy7EPAtpJGt5TdDBxWBmv/iOpb73CuB54sg7SblBheJ+mNHXuHK/pjSW9XdSXWR6lO51wLXEeV6D4uaYMyyH8E1amtoTxENcbSbzOqf9Z9UF0cQNWzaMv2g1QXDHxN0tYlhv3K5vOAEyW9SZXNJB0uaQtJu0p6a0nsf6DqSS0fopl4GUqyiK5geyFwFNWpnz6qb7H/AKxnewnwYeD7wGPAu4BZLcf+FvgesKCcHtkeuBC4hWoA9udUA7bDtb+c6p/yHlSDzY8A3wRGD3fcavgx1cDzY8BfAW8v4wPPAUdSjRs8AnwN+OvyHodyPtVYweOSLrN9B/DPwG+oEsnrgWtWIra/ohqD+S3VgPZHAWz3UI1bfLXEPY9qsByqZH5mifl3wCuofpexlshNeREjTNKngVfbfs+ajiWirvQsIiKirSSLiIhoK6ehIiKirfQsIiKirZftBGoDbbfddh4/fvyaDiMi4mXlhhtueMT2mHb7rTXJYvz48fT09KzpMCIiXlYk3Vdnv5yGioiItpIsIiKirSSLiIhoK8kiIiLaSrKIiIi2kiwiIqKtJIuIiGgrySIiItpKsoiIiLbWmju4V9dVV9V54uTK2X//TNIYEWuH9CwiIqKtRpOFpKmS7pI0T9Kpg2w/UdJtkm6W9CtJk0v5eEnPlPKbJX2jyTgjImJ4jZ2GkjQKOBs4COgF5kiaVZ4P3O8i298o+x8JfAWYWrbNt71HU/FFRER9TfYs9gbm2V5QHkI/EziqdQfbT7asbgbkJH9ERBdqMlnsACxsWe8tZSuQdJKk+cBZwIdbNk2QdJOkqyX96WANSDpBUo+knr6+vk7GHhERLZpMFoNdXvSSnoPts23vDHwCOL0UPwjsaHtP4BTgIklbDnLsuban2J4yZkzbZ3dERMQqajJZ9ALjWtbHAouG2X8mcDSA7WdtP1qWbwDmA7s0FGdERLTRZLKYA0ySNEHShsA0YFbrDpImtaweDtxdyseUAXIkTQQmAQsajDUiIobR2NVQtpdJOhm4HBgFzLA9V9J0oMf2LOBkSQcCS4HHgOPK4fsB0yUtA5YDJ9pe3FSsERExvEbv4LY9G5g9oOyMluWPDHHcpcClTcYWERH15Q7uiIhoK8kiIiLaSrKIiIi2kiwiIqKtJIuIiGgrySIiItpKsoiIiLaSLCIioq0ki4iIaCvJIiIi2kqyiIiIthqdGype6qqrBnvMx6rbf/88XDAimpeeRUREtJVkERERbSVZREREW0kWERHRVpJFRES0lWQRERFtJVlERERbjSYLSVMl3SVpnqRTB9l+oqTbJN0s6VeSJrdsO60cd5ekQ5qMMyIihtdYspA0CjgbOBSYDLyzNRkUF9l+ve09gLOAr5RjJwPTgN2AqcDXSn0REbEGNNmz2BuYZ3uB7eeAmcBRrTvYfrJldTOg/3bko4CZtp+1fQ8wr9QXERFrQNtkIelYSVuU5dMl/VDSXjXq3gFY2LLeW8oG1n+SpPlUPYsPr8yxERExMur0LP7J9hJJbwYOAS4Avl7juMEmQXrJREa2z7a9M/AJ4PSVOVbSCZJ6JPX09fXVCCkiIlZFnWSxvPw8HPi67R8DG9Y4rhcY17I+Flg0zP4zgaNX5ljb59qeYnvKmDFjaoQUERGrok6yeEDSOcA7gNmSNqp53BxgkqQJkjakGrCe1bqDpEktq4cDd5flWcA0SRtJmgBMAq6v0WZERDSgzhTl76C6IunLth+X9CrgH9odZHuZpJOBy4FRwAzbcyVNB3pszwJOlnQgsBR4DDiuHDtX0veBO4BlwEm2lw/aUERENK5tsrD9tKSHgTdTffNfxos9gHbHzgZmDyg7o2X5I8Mc+3ng83XaiYiIZtW5GupTVIPPp5WiDYDvNBlURER0lzpjD28DjgSeArC9CNiiyaAiIqK71EkWz9k25dJVSZs1G1JERHSbOsni++VqqK0kfQD4T+C8ZsOKiIhuUmeA+8uSDgKeBHYFzrB9ReORRURE16hz6SwlOSRBRESso9omC0lLeHGqjQ2proZ6yvaWTQYWERHdo85pqBWufJJ0NJkBNiJinbLSU5Tbvgx4awOxREREl6pzGurtLavrAVMYZAbYiIhYe9UZ4D6iZXkZcC8DHmIUERFrtzpjFu8biUAiIqJ7DZksJP0bw5xusv3hobZFRMTaZbieRc+IRREREV1tyGRh+4KRDCQiIrpXnauhxlBNUT4Z2Li/3HYun42IWEfUuc/iu8CdwATgM1RXQ81pMKaIiOgydZLFtrbPB5bavtr2+4F9Go4rIiK6SJ37LJaWnw9KOhxYBIxtLqSIiOg2dZLF5ySNBv438G/AlsDfNxpVRER0lTrJ4jrbTwBPAH+2MpVLmgr8CzAK+KbtMwdsPwX4G6o7w/uA99u+r2xbDtxWdr3f9pEr03ZERHROnTGLX0v6uaTjJW1dt2JJo4CzgUOprqR6p6TJA3a7CZhie3fgEuCslm3P2N6jvJIoIiLWoLbJwvYk4HRgN+AGST+V9J4ade8NzLO9wPZzwEwGzCll+5e2ny6r15KxkIiIrlRrinLb19s+hSoBLAbq3LC3A7CwZb23lA3leOBnLesbS+qRdG15hsZLSDqh7NPT19dXI6SIiFgVdW7K2xJ4GzAN2Bn4EfUefqRBygada6r0VKYAb2kp3tH2IkkTgV9Ius32/BUqs88FzgWYMmVKpk2PiGhInQHuW4DLgOm2f7MSdfcC41rWx1JddrsCSQcC/wi8xfaz/eW2F5WfCyRdBewJzB94fERENK9Osphoe1W+tc8BJkmaADxA1TN5V+sOkvYEzgGm2n64pXxr4Gnbz0raDtiXFQe/IyJiBNV5nsUqnd6xvUzSycDlVJfOzrA9V9J0oMf2LOBLwObADyTBi5fIvhY4R9LzVOMqZ9q+Y1XiiIiI1VenZ7HKbM8GZg8oO6Nl+cAhjvs18PomY4uIiPpqXQ0VERHrtrbJQtIukq6UdHtZ313S6c2HFhER3aJOz+I84DTKhIK2b6UarI6IiHVEnWSxqe3rB5QtayKYiIjoTnWSxSOSdqbcUCfpGODBRqOKiIiuUudqqJOo7pJ+jaQHgHuAOnNDRUTEWqLOfRYLgAMlbQasZ3tJ82FFREQ3qXM11BckbWX7KdtLJG0t6XMjEVxERHSHOmMWh9p+vH/F9mPAYc2FFBER3aZOshglaaP+FUmbABsNs39ERKxl6gxwfwe4UtK3qK6Iej/1nmcRERFriToD3GdJug04gOoZFZ+1fXnjkUVERNeoNZGg7Z+x4lPsIiJiHVLnaqi3S7pb0hOSnpS0RNKTIxFcRER0hzo9i7OAI2zf2XQwERHRnepcDfVQEkVExLqtTs+iR9LFVM/hbn1G9g8biyoiIrpKnWSxJfA0cHBLmYEki4iIdUSdS2ffNxKBRERE92qbLCRtDBwP7AZs3F9u+/0NxhUREV2kzgD3hcAfAYcAVwNjgVozz0qaKukuSfMknTrI9lMk3SHp1vLo1p1ath1XLtm9W9Jx9d5OREQ0oU6yeLXtfwKesn0BcDjw+nYHSRoFnA0cCkwG3ilp8oDdbgKm2N4duITqMl0kbQN8CngTsDfwKUlb13tLERHRaXWSxdLy83FJrwNGA+NrHLc3MM/2AtvPATOBo1p3sP1L20+X1Wupei1Q9WKusL24zHJ7BTC1RpsREdGAOsni3PKt/nRgFnAH8MUax+0ALGxZ7y1lQzmeF6cUqXWspBMk9Ujq6evrqxFSRESsijqXzl5Zvt3/FzARQNKEGsdpkDIPuqP0HmAK8JaVOdb2uVSPfGXKlCmD1h0REauvTs/i0kHKLqlxXC8wrmV9LLBo4E6SDgT+ETjS9rMrc2xERIyMIXsWkl5DdbnsaElvb9m0JS2X0A5jDjCp9EIeAKYB7xrQxp7AOcBU2w+3bLoc+ELLoPbBwGk12oyIiAYMdxpqV+DPga2AI1rKlwAfaFex7WWSTqb6xz8KmGF7rqTpQI/tWcCXgM2BH0gCuN/2kbYXS/osVcIBmG578Uq+t4iI6JAhk4XtH0v6KfAJ219YlcptzwZmDyg7o2X5wGGOnQHMWJV2IyKis4Yds7C9HDhohGKJiIguVedqqF9L+ipwMfBUf6HtGxuLKiIiukqdZPEn5ef0ljIDb+18OBER0Y3qzDr7ZyMRSEREdK86z+AeLekr/XdKS/pnSaNHIriIiOgOdW7Km0F1uew7yutJ4FtNBhUREd2lzpjFzrb/omX9M5JubiqgiIjoPnV6Fs9IenP/iqR9gWeaCykiIrpNnZ7Fh4ALyjiFgMVAHkYUEbEOqXM11M3AGyRtWdafbDyqiIjoKnWuhtpW0r8CVwG/lPQvkrZtPLKIiOgadcYsZgJ9wF8Ax5Tli5sMKiIiukudMYttbH+2Zf1zko5uKqCIiOg+dXoWv5Q0TdJ65fUO4N+bDiwiIrpHnWTxQeAi4LnymgmcImmJpAx2R0SsA+pcDbXFSAQSERHdq86YBZJ2B8a37m/7hw3FFBERXaZtspA0A9gdmAs8X4oNJFlERKwj6vQs9rE9ufFIIiKia9UZ4P6NpCSLiIh1WJ1kcQFVwrhL0q2SbpN0a53KJU0tx82TdOog2/eTdKOkZZKOGbBtuaSby2tWvbcTERFNqHMaagbwV8BtvDhm0ZakUcDZwEFALzBH0izbd7Tsdj/wXuBjg1TxjO096rYXERHNqZMs7re9Kt/s9wbm2V4AIGkmcBTwQrKwfW/ZVjsJRUTEyKuTLH4r6SLgJ8Cz/YU1Lp3dAVjYst4LvGklYttYUg+wDDjT9mUDd5B0AnACwI477rgSVUdExMqokyw2oUoSB7eU1bl0VoOUuWZcADvaXiRpIvALSbfZnr9CZfa5wLkAU6ZMWZm6IyJiJdS5g/t9q1h3LzCuZX0ssKjuwbYXlZ8LJF0F7AnMH/agiIhoxJDJQtK/MUxPwPaH29Q9B5gkaQLwADANeFedoCRtDTxt+1lJ2wH7AmfVOTYiIjpvuJ5Fz+pUbHuZpJOBy4FRwAzbcyVNB3psz5L0RuBHwNbAEZI+Y3s34LXAOWXgez2qMYs7hmgqIiIaNmSysH3B6lZuezYwe0DZGS3Lc6hOTw087tfA61e3/YiI6Iw6N+VFRMQ6LskiIiLaSrKIiIi22iYLSbtIulLS7WV9d0mnNx9aRER0izo9i/OA04ClALZvpboMNiIi1hF1ksWmtq8fULasiWAiIqI71UkWj0jamXKDXplK/MFGo4qIiK5SZ26ok6jmX3qNpAeAe4B3NxpVRER0lWGThaT1gCm2D5S0GbCe7SUjE1pERHSLYU9D2X4eOLksP5VEERGxbqozZnGFpI9JGidpm/5X45FFRETXqDNm8f7y86SWMgMTOx9ORER0ozrPs5gwEoFERET3apssJP31YOW2/1/nw4mIiG5U5zTUG1uWNwYOAG4EkiwiItYRdU5D/V3ruqTRwIWNRRQREV1nVWadfRqY1OlAIiKie9UZs/gJLz6Lez1gMvCDJoOKiIjuUmfM4ssty8uA+2z3NhRPRER0oTqnoQ6zfXV5XWO7V9IXG48sIiK6Rp1kcdAgZYfWqVzSVEl3SZon6dRBtu8n6UZJy8pstq3bjpN0d3kdV6e9iIhoxpCnoSR9CPhbYKKkW1s2bQFc065iSaOAs6mSTS8wR9Is23e07HY/8F7gYwOO3Qb4FDCFarzkhnLsY3XeVEREdNZwYxYXAT8D/g/Q2itYYntxjbr3BubZXgAgaSZwFPBCsrB9b9n2/IBjDwGu6G9H0hXAVOB7NdqNiIgOG/I0lO0nbN9r+5227wOeofqWv7mkHWvUvQOwsGW9t5TVUetYSSdI6pHU09fXV7PqiIhYWW3HLCQdIeluqoceXQ3cS9XjaHvoIGUepGyVj7V9ru0ptqeMGTOmZtUREbGy6gxwfw7YB/ifMqngAdQYs6DqDYxrWR8LLKoZ1+ocGxERHVYnWSy1/SiwnqT1bP8S2KPGcXOASZImSNoQmAbMqhnX5cDBkraWtDVwcCmLiIg1oM5NeY9L2hz4b+C7kh6mujlvWLaXSTqZ6p/8KGCG7bmSpgM9tmdJeiPwI2Br4AhJn7G9m+3Fkj5LlXAAptccVI+IiAbUSRZHUQ1ufxR4NzAamF6nctuzgdkDys5oWZ5DdYppsGNnADPqtBMREc2qM+vsU5J2AibZvkDSplQ9hYiIWEfUuRrqA8AlwDmlaAfgsiaDioiI7lJngPskYF/gSQDbdwOvaDKoiIjoLnWSxbO2n+tfkbQ+9e+XiIiItUCdZHG1pE8Cm0g6iOpZFj9pNqyIiOgmdZLFqUAfcBvwQaqrm05vMqiIiOguw806u6Pt+20/D5xXXhERsQ4armfxwhVPki4dgVgiIqJLDZcsWifzm9h0IBER0b2GSxYeYjkiItYxw93B/QZJT1L1MDYpy5R1296y8egiIqIrDJksbGdKj4iIAOpdOhsREeu4JIuIiGgrySIiItpKsoiIiLaSLCIioq0ki4iIaKvOY1XjZeaqq9R+p5W0//4vvS9zpNqJiDUvPYuIiGir0WQhaaqkuyTNk3TqINs3knRx2X6dpPGlfLykZyTdXF7faDLOiIgYXmOnoSSNAs4GDgJ6gTmSZtm+o2W344HHbL9a0jTgi8Bflm3zbe/RVHwREVFfk2MWewPzbC8AkDQTOApoTRZHAZ8uy5cAX5XU+RPh8bKWsZGINa/J01A7AAtb1ntL2aD72F4GPAFsW7ZNkHSTpKsl/elgDUg6QVKPpJ6+vr7ORh8RES9oMlkM9nVw4Ne5ofZ5ENjR9p7AKcBFkl4yy63tc21PsT1lzJgxqx1wREQMrslk0QuMa1kfCywaah9J6wOjgcW2n7X9KIDtG4D5wC4NxhoREcNocsxiDjBJ0gTgAWAa8K4B+8wCjgN+AxwD/MK2JY2hShrLJU0EJgELGow1ouNjI7k3JdYmjSUL28sknQxcDowCZtieK2k60GN7FnA+cKGkecBiqoQCsB8wXdIyYDlwou3FTcUaERHDa/QObtuzgdkDys5oWf4DcOwgx10KXNpkbBERUV/u4I6IiLaSLCIioq0ki4iIaCvJIiIi2kqyiIiItpIsIiKirSSLiIhoK0/Ki1gL5U7x6LT0LCIioq30LCJilaUHs+5IzyIiItpKsoiIiLaSLCIioq0ki4iIaCvJIiIi2kqyiIiItpIsIiKirSSLiIhoK8kiIiLayh3cEdH1On2neO4SX3mN9iwkTZV0l6R5kk4dZPtGki4u26+TNL5l22ml/C5JhzQZZ0REDK+xnoWkUcDZwEFALzBH0izbd7TsdjzwmO1XS5oGfBH4S0mTgWnAbsD2wH9K2sX28qbijYh1W+a5Gl6Tp6H2BubZXgAgaSZwFNCaLI4CPl2WLwG+KkmlfKbtZ4F7JM0r9f2mwXgjIhr3ck1KTSaLHYCFLeu9wJuG2sf2MklPANuW8msHHLvDwAYknQCcUFZ/L+muzoQ+rO2AR+rtulp/FDXbWe0/vJFop8s+s5FqJ7+b7m0nv5sWO9XZqclkMVj0A9PfUPvUORbb5wLnrnxoq05Sj+0paae72kg73dtG2uneNlZGkwPcvcC4lvWxwKKh9pG0PjAaWFzz2IiIGCFNJos5wCRJEyRtSDVgPWvAPrOA48ryMcAvbLuUTytXS00AJgHXNxhrREQMo7HTUGUM4mTgcmAUMMP2XEnTgR7bs4DzgQvLAPZiqoRC2e/7VIPhy4CTuuhKqJE67bU2tbM2vZe1rZ216b2sbe2M6Cn2dlR9kY+IiBhapvuIiIi2kiwiIqKtJIthSJoh6WFJt7eUHStprqTnJTVyWZukeyXdJulmST0N1L9rqbv/9aSkj3a6ndLWsFO+dLitUZJukvTTDtY52N/ANpKukHR3+bl1B9sbJ+mXku4sf2cf6VTdg7S1laRLJP22tPe/GmrnI5JuL++nqb+zvy/13y7pe5I2bqidl/w9NNDGxpKul3RLeU+faaqtlWI7ryFewH7AXsDtLWWvBXYFrgKmNNTuvcB2I/QeRwG/A3ZqqO75wERgQ+AWYHKD7+UU4CLgpw3/DZwFnFqWTwW+2MH2XgXsVZa3AP6nqc8MuAD4m7K8IbBVA228Drgd2JTqgpr/BCZ1uI0dgHuATcr694H3NvSZveTvoYE2BGxeljcArgP2aaq9uq/0LIZh+7+ortJqLbvT9kjcKT5SDgDm276vgbpfmPLF9nNA/5QvHSdpLHA48M1O1jvY3wDVe7igLF8AHN3B9h60fWNZXgLcySCzF6wuSVtS/eM7v7T1nO3HO90O1Zera20/bXsZcDXwtgbaWR/YpNyvtSkN3Zc1xN9Dp9uw7d+X1Q3Ka41fiZRk0Z0M/FzSDWVKkyZNA77XUN2DTfnS8X98xf8FPg4831D9rV5p+0Go/rkDr2iikTIL855U3yw7bSLQB3yrnLr7pqTNGmjndmA/SdtK2hQ4jBVvuF1tth8AvgzcDzwIPGH7551sY6SVU6o3Aw8DV9hu4m9gpSRZdKd9be8FHAqcJGm/JhopN0seCfygifqpOW3Lajci/TnwsO0bOl33miJpc+BS4KO2n2ygifWpTqd83faewFNUp9Q6yvadVLNJXwH8B9WpyGWdbKOMGR0FTKCapXozSe/pZBsjzfZy23tQzV6xt6TXremYkiy6kO1F5efDwI+oTuc04VDgRtsPNVT/SE3bsi9wpKR7qU51vVXSdxpop99Dkl4FUH4+3MnKJW1AlSi+a/uHnay7RS/Q2/KN9RKq5NFxts+3vZft/ahO4dzd4SYOBO6x3Wd7KfBD4E863MYaUU4NXgVMXcOhJFl0G0mbSdqifxk4mKor34R30twpKKg35ctqs32a7bG2x5c2fmG7yW+WrdPUHAf8uFMVlyn6zwfutP2VTtU7kO3fAQsl7VqKDmDFxwd0jKRXlJ87Am+n839z9wP7SNq0fH4HUI31vCxJGiNpq7K8CVUy/O2ajYpcDTXci+qP+kFgKdU3seOpBud6gWeBh4DLO9zmRKqu+i3AXOAfG3pvmwKPAqMb/gwPo7qiZ35T72VAe/vT2auhBvsb2Ba4kuob8pXANh1s781Up+puBW4ur8Ma+qz2AHpKW5cBWzfUzn9TJaJbgAMaauMzVP9QbwcuBDZqqJ2X/D000MbuwE3l93I7cEYT72VlX5nuIyIi2sppqIiIaCvJIiIi2kqyiIiItpIsIiKirSSLiIhoK8kiYgBJywfMyjt+FerYStLfdj66iDUjl85GDCDp97Y3X806xlPd77FS0zRIGuXueYRwxAvSs4iooUzs9iVJcyTdKumDpXxzSVdKurE8g6R/Vt0zgZ1Lz+RLkvY+FhUqAAABz0lEQVRvfc6GpK9Kem9ZvlfSGZJ+BRwraWdJ/1EmkvxvSa8p+x1bntdwi6T/GtlPINZ166/pACK60CZlxk+o5hx6G9Wd20/YfqOkjYBrJP2calbdt9l+UtJ2wLWSZlFNyvc6V5PBIWn/Nm3+wfaby75XAifavlvSm4CvAW8FzgAOsf1A/3QQESMlySLipZ7p/yff4mBgd0nHlPXRwCSqKR++UGYGfp5qCvZXrkKbF8MLs83+CfCDapojADYqP68Bvi3p+1ST5UWMmCSLiHoE/J3ty1corE4ljQH+2PbSMvPtYI/0XMaKp30H7vNU+bke8PggyQrbJ5aexuHAzZL2sP3oqryZiJWVMYuIei4HPlSmD0fSLmVW4NFUz9JYKunPgJ3K/kuoHova7z5gsqSNJI2mmhn1JVw9u+IeSceWdiTpDWV5Z9vX2T4DeIQOP0QoYjjpWUTU801gPHBjmQa7j+pxqt8FfiKph2qG2N8C2H5U0jWSbgd+ZvsfyumjW6lmq71pmLbeDXxd0ulUj9ScSTVj65ckTaLq5VxZyiJGRC6djYiItnIaKiIi2kqyiIiItpIsIiKirSSLiIhoK8kiIiLaSrKIiIi2kiwiIqKt/w85nDh5oHelUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting feature names for the results\n",
    "feature_names = ['crim', 'zn', 'indus', 'chas', 'nox',\n",
    "                 'rooms', 'age', 'dis', 'rad',\n",
    "                 'tax', 'ptratio', 'lstat']\n",
    "\n",
    "f_name = feature_names\n",
    "\n",
    "# getting model data\n",
    "nrows = model_data.shape[0]-1\n",
    "ncols = model_data.shape[1]\n",
    "\n",
    "X = model_data[0:nrows,1:ncols]\n",
    "y = model_data[0:nrows,0]\n",
    "\n",
    "# using rf2 defined above as\n",
    "# RandomForestRegressor(n_estimators = 100,\n",
    "# random_state = RANDOM_SEED, max_features = 5)\n",
    "\n",
    "rf2.fit(X, y)\n",
    "\n",
    "f_imp = rf2.feature_importances_\n",
    "std = np.std([f_imp for tree in rf2.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(f_imp)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, feature_names[indices[f]],\n",
    "                                   f_imp[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), f_imp[indices],\n",
    "       color=\"y\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylabel(\"Feature Importance values\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at feature importance statistics it is observed that the most important features in Boston data are lstat - \"Percentage of population of lower socio-economic status\" and rooms - \"Average number of rooms per home\". These two attributes are more important in predicting the Median Value of homes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
